{
  "paper_title": "attention_is_all",
  "algorithm_name": "Multi-Head Self-Attention",
  "summary": "Computes attention over a single sequence using multiple independent attention heads to capture different dependency types.",
  "steps": [
    {
      "step_id": "step_01",
      "title": "SETUP",
      "description": "Import required libraries and define constants for the implementation.",
      "step_type": "setup",
      "inputs": [],
      "outputs": [
        "torch",
        "math",
        "random",
        "SEED",
        "DEVICE"
      ],
      "dependencies": [],
      "code_prompt": "Import torch, math, random. Set a random seed for reproducibility. Define DEVICE as 'cuda' if torch.cuda.is_available() else 'cpu'."
    },
    {
      "step_id": "step_02",
      "title": "DATA_GENERATION",
      "description": "Create synthetic query, key, and value tensors for testing the multi-head self-attention mechanism.",
      "step_type": "data_generation",
      "inputs": [],
      "outputs": [
        "Q",
        "K",
        "V",
        "n",
        "d_model",
        "h",
        "d_k",
        "d_v"
      ],
      "dependencies": [
        "step_01"
      ],
      "code_prompt": "Set n=8 (sequence length), d_model=512 (model dimension), h=8 (heads), d_k=d_v=d_model//h=64. Generate random Q, K, V tensors of shape (n, d_model) on DEVICE."
    },
    {
      "step_id": "step_03",
      "title": "CORE_LOGIC: MultiHeadSelfAttention class",
      "description": "Implement a PyTorch nn.Module for multi-head self-attention with learnable projection matrices.",
      "step_type": "core_logic",
      "inputs": [
        "d_model",
        "h",
        "d_k",
        "d_v"
      ],
      "outputs": [
        "MultiHeadSelfAttention"
      ],
      "dependencies": [
        "step_01"
      ],
      "code_prompt": "Define class MultiHeadSelfAttention(nn.Module) with __init__ taking d_model, h, dropout=0.1. Create nn.Linear layers for w_q, w_k, w_v (each output d_model) and w_o (input h*d_v, output d_model). Implement forward(self, Q, K, V, mask=None) following the three paper steps: split heads, scaled dot-product attention per head, concat heads and project."
    },
    {
      "step_id": "step_04",
      "title": "INFERENCE",
      "description": "Instantiate the module and run forward pass to obtain context vectors.",
      "step_type": "inference",
      "inputs": [
        "Q",
        "K",
        "V",
        "MultiHeadSelfAttention"
      ],
      "outputs": [
        "output"
      ],
      "dependencies": [
        "step_02",
        "step_03"
      ],
      "code_prompt": "Instantiate model=MultiHeadSelfAttention(d_model, h).to(DEVICE). Run output=model(Q,K,V) and print output shape."
    },
    {
      "step_id": "step_05",
      "title": "VALIDATION",
      "description": "Check that the output shape matches expected dimensions and that attention weights sum to one.",
      "step_type": "validation",
      "inputs": [
        "output",
        "n",
        "d_model"
      ],
      "outputs": [
        "assertions"
      ],
      "dependencies": [
        "step_04"
      ],
      "code_prompt": "Assert output.shape == (n, d_model). Optionally extract attention weights from one head and assert that their row sums are close to 1.0 (within 1e-5)."
    },
    {
      "step_id": "step_06",
      "title": "VISUALIZATION",
      "description": "Plot an attention heatmap for one head to visualize dependencies.",
      "step_type": "visualization",
      "inputs": [
        "MultiHeadSelfAttention",
        "Q",
        "K",
        "V"
      ],
      "outputs": [
        "heatmap_plot"
      ],
      "dependencies": [
        "step_04"
      ],
      "code_prompt": "Extract attention weights from the first head (shape n√ón). Use matplotlib.pyplot.imshow to display the heatmap with labels 'Query position' and 'Key position'. Save or show the plot."
    }
  ]
}